{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## acquire\n",
    "\n",
    "Goal: Predict the logerror\n",
    "\n",
    "Zillow data:\n",
    "- 2017 data\n",
    "- Latest transaction per property id only.\n",
    "- The logerror from that latest transaction.\n",
    "- All fields related to the properties.\n",
    "- Gather descriptions from the lookup tables.\n",
    "- Only properties with latitude and longitude.\n",
    "- Only single family homes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PowerTransformer, LabelEncoder, OneHotEncoder, QuantileTransformer, MinMaxScaler\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import acquire\n",
    "import summarize\n",
    "import prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 85224 entries, 0 to 85223\n",
      "Data columns (total 60 columns):\n",
      "id                              85224 non-null int64\n",
      "parcelid                        85224 non-null int64\n",
      "airconditioningtypeid           28311 non-null float64\n",
      "architecturalstyletypeid        261 non-null float64\n",
      "basementsqft                    42 non-null float64\n",
      "bathroomcnt                     85224 non-null float64\n",
      "bedroomcnt                      85224 non-null float64\n",
      "buildingclasstypeid             0 non-null object\n",
      "buildingqualitytypeid           53810 non-null float64\n",
      "calculatedbathnbr               85207 non-null float64\n",
      "decktypeid                      654 non-null float64\n",
      "finishedfloor1squarefeet        6821 non-null float64\n",
      "calculatedfinishedsquarefeet    85224 non-null float64\n",
      "finishedsquarefeet12            85035 non-null float64\n",
      "finishedsquarefeet13            2 non-null float64\n",
      "finishedsquarefeet15            29 non-null float64\n",
      "finishedsquarefeet50            6821 non-null float64\n",
      "finishedsquarefeet6             158 non-null float64\n",
      "fips                            85224 non-null float64\n",
      "fireplacecnt                    9551 non-null float64\n",
      "fullbathcnt                     85207 non-null float64\n",
      "garagecarcnt                    29680 non-null float64\n",
      "garagetotalsqft                 29680 non-null float64\n",
      "hashottuborspa                  2358 non-null float64\n",
      "heatingorsystemtypeid           55685 non-null float64\n",
      "latitude                        85224 non-null float64\n",
      "longitude                       85224 non-null float64\n",
      "lotsizesquarefeet               75486 non-null float64\n",
      "poolcnt                         17726 non-null float64\n",
      "poolsizesum                     964 non-null float64\n",
      "pooltypeid10                    1154 non-null float64\n",
      "pooltypeid2                     1204 non-null float64\n",
      "pooltypeid7                     16522 non-null float64\n",
      "propertycountylandusecode       85224 non-null object\n",
      "propertylandusetypeid           85224 non-null float64\n",
      "propertyzoningdesc              54068 non-null object\n",
      "rawcensustractandblock          85224 non-null float64\n",
      "regionidcity                    83552 non-null float64\n",
      "regionidcounty                  85224 non-null float64\n",
      "regionidneighborhood            33262 non-null float64\n",
      "regionidzip                     85209 non-null float64\n",
      "roomcnt                         85224 non-null float64\n",
      "storytypeid                     42 non-null float64\n",
      "threequarterbathnbr             12007 non-null float64\n",
      "typeconstructiontypeid          299 non-null float64\n",
      "unitcnt                         54142 non-null float64\n",
      "yardbuildingsqft17              2635 non-null float64\n",
      "yardbuildingsqft26              93 non-null float64\n",
      "yearbuilt                       85186 non-null float64\n",
      "numberofstories                 20264 non-null float64\n",
      "fireplaceflag                   222 non-null float64\n",
      "structuretaxvaluedollarcnt      85162 non-null float64\n",
      "taxvaluedollarcnt               85223 non-null float64\n",
      "assessmentyear                  85224 non-null float64\n",
      "landtaxvaluedollarcnt           85223 non-null float64\n",
      "taxamount                       85219 non-null float64\n",
      "taxdelinquencyflag              1614 non-null object\n",
      "taxdelinquencyyear              1614 non-null float64\n",
      "censustractandblock             84996 non-null float64\n",
      "logerror                        85224 non-null float64\n",
      "dtypes: float64(54), int64(2), object(4)\n",
      "memory usage: 39.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df = acquire.get_zillow_data()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only single family"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.propertylandusetypeid == 261]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare \n",
    "\n",
    "### Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- remove columns with > 99% missing and rows  > 40% missing\n",
    "- aggregate pool information: use all pool and spa columns to compute a single boolean attribute of `has_pool`\n",
    "- fill with 0: taxdelinquencyflag, fireplacecnt, garagecarcnt and convert them to boolean\n",
    "- After doing all that, then remove all columns with > 5% missing, and following that, rows with > 99% missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 58419 entries, 0 to 85221\n",
      "Data columns (total 31 columns):\n",
      "id                              58419 non-null int64\n",
      "parcelid                        58419 non-null int64\n",
      "bathroomcnt                     58419 non-null float64\n",
      "bedroomcnt                      58419 non-null float64\n",
      "calculatedbathnbr               58419 non-null float64\n",
      "calculatedfinishedsquarefeet    58419 non-null float64\n",
      "finishedsquarefeet12            58419 non-null float64\n",
      "fips                            58419 non-null float64\n",
      "fullbathcnt                     58419 non-null float64\n",
      "latitude                        58419 non-null float64\n",
      "longitude                       58419 non-null float64\n",
      "lotsizesquarefeet               58419 non-null float64\n",
      "propertycountylandusecode       58419 non-null object\n",
      "propertylandusetypeid           58419 non-null float64\n",
      "rawcensustractandblock          58419 non-null float64\n",
      "regionidcity                    58419 non-null float64\n",
      "regionidcounty                  58419 non-null float64\n",
      "regionidzip                     58419 non-null float64\n",
      "roomcnt                         58419 non-null float64\n",
      "yearbuilt                       58419 non-null float64\n",
      "structuretaxvaluedollarcnt      58419 non-null float64\n",
      "taxvaluedollarcnt               58419 non-null float64\n",
      "assessmentyear                  58419 non-null float64\n",
      "landtaxvaluedollarcnt           58419 non-null float64\n",
      "taxamount                       58419 non-null float64\n",
      "censustractandblock             58419 non-null float64\n",
      "logerror                        58419 non-null float64\n",
      "has_pool                        58419 non-null float64\n",
      "is_taxdelinquent                58419 non-null float64\n",
      "has_fireplace                   58419 non-null float64\n",
      "has_garage                      58419 non-null float64\n",
      "dtypes: float64(28), int64(2), object(1)\n",
      "memory usage: 14.3+ MB\n"
     ]
    }
   ],
   "source": [
    "# remove columns with > 99% missing and rows  > 40% missing\n",
    "df = prepare.handle_missing_values(df, prop_required_column = .01, prop_required_row = .40)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['hashottuborspa', 'poolcnt', 'poolsizesum', 'pooltypeid2',\\n       'pooltypeid7'],\\n      dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-cad0b4564b14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpool_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'hashottuborspa'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'poolcnt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'poolsizesum'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pooltypeid2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pooltypeid7'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# fill all missing values with 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mpool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpool_cols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m# where there is a value in one or more of the pool attributes, assign a 1 to a new col named 'pool'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'has_pool'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2932\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2933\u001b[0m             indexer = self.loc._convert_to_indexer(key, axis=1,\n\u001b[0;32m-> 2934\u001b[0;31m                                                    raise_missing=True)\n\u001b[0m\u001b[1;32m   2935\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_convert_to_indexer\u001b[0;34m(self, obj, axis, is_setter, raise_missing)\u001b[0m\n\u001b[1;32m   1352\u001b[0m                 kwargs = {'raise_missing': True if is_setter else\n\u001b[1;32m   1353\u001b[0m                           raise_missing}\n\u001b[0;32m-> 1354\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1355\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1356\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[0;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1159\u001b[0m         self._validate_read_indexer(keyarr, indexer,\n\u001b[1;32m   1160\u001b[0m                                     \u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis_number\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1161\u001b[0;31m                                     raise_missing=raise_missing)\n\u001b[0m\u001b[1;32m   1162\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[0;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1244\u001b[0m                 raise KeyError(\n\u001b[1;32m   1245\u001b[0m                     u\"None of [{key}] are in the [{axis}]\".format(\n\u001b[0;32m-> 1246\u001b[0;31m                         key=key, axis=self.obj._get_axis_name(axis)))\n\u001b[0m\u001b[1;32m   1247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1248\u001b[0m             \u001b[0;31m# We (temporarily) allow for some missing keys with .loc, except in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Index(['hashottuborspa', 'poolcnt', 'poolsizesum', 'pooltypeid2',\\n       'pooltypeid7'],\\n      dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "# aggregate pool information: use all pool and spa columns to compute a single attribute of pool_spa\n",
    "# # gather pool columns\n",
    "# pool_cols = ['hashottuborspa', 'poolcnt', 'poolsizesum', 'pooltypeid2', 'pooltypeid7']\n",
    "# # fill all missing values with 0\n",
    "# pool = df[pool_cols].fillna(0)\n",
    "# # where there is a value in one or more of the pool attributes, assign a 1 to a new col named 'pool'\n",
    "# pool.loc[pool.sum(axis=1)>0, 'has_pool'] = 1\n",
    "# # append the new column to our original dataframe and remove the original pool columns \n",
    "# df = df.join(pool[['has_pool']])\n",
    "\n",
    "# fill with 0\n",
    "df.loc[df.taxdelinquencyflag == 'Y', 'is_taxdelinquent'] = 1\n",
    "df.loc[df.fireplacecnt > 0, 'has_fireplace'] = 1\n",
    "df.loc[df.garagecarcnt > 0, 'has_garage'] = 1\n",
    "fill_with_0 = ['has_garage', 'has_fireplace', 'has_pool', 'is_taxdelinquent']\n",
    "df[fill_with_0] = df[fill_with_0].fillna(0)\n",
    "\n",
    "# remove columns where > 5% missing and rows where > 99% missing\n",
    "df = prepare.handle_missing_values(df, prop_required_column = .95, prop_required_row = .99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable Changes\n",
    "\n",
    "Are there any instances where taxvaluedollarcnt is not equal to the sum of the land tax value and the structure tax value? (landtaxvaluedollarcnt + structuretaxvaluedollarcnt). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where((df.taxvaluedollarcnt - (df.landtaxvaluedollarcnt + df.structuretaxvaluedollarcnt)) != 0)\n",
    "# add taxvaluedollarcnt to list to drop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No. I will attempt to reduce the dependency between variables and extracting the most unique information from each. \n",
    "\n",
    "- `land_dollar_per_sqft`: a land dollar per sqft (landtaxvaluedollarcnt/lotsizesquarefeet)\n",
    "- `structure_dollar_per_sqft`: structuretaxvaluedollarcnt/calculatedfinishedsquarefeet\n",
    "- `tax_rate`: taxvaluedollarcnt/taxamount\n",
    "- compute `living_area_sqft` by subtracting estimated square feet from bedrooms (121: 11x11) and bathrooms (36: 6x6)\n",
    "- compute `bedbath_index` where multiple bedrooms by a weight of 2, full baths by weight of 1, half/three-quarter baths by weight of .5, then sum them all together.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['structure_dollar_per_sqft'] = df.structuretaxvaluedollarcnt/df.calculatedfinishedsquarefeet\n",
    "df['land_dollar_per_sqft'] = df.landtaxvaluedollarcnt/df.lotsizesquarefeet\n",
    "df['living_area_sqft'] = df.calculatedfinishedsquarefeet - (df.bedroomcnt*121 + df.bathroomcnt*36)\n",
    "df['tax_rate'] = df.taxvaluedollarcnt/df.taxamount\n",
    "df['bedbath_index'] = df.bedroomcnt*2 + df.fullbathcnt + .5*(df.bathroomcnt-df.fullbathcnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- turn yearbuilt into age (from present)\n",
    "- reduce regionidcity into the top 5 cities and the others assign to a catch-all id.\n",
    "- take the first 3 digits of zip to reduce the variance in zipcode \n",
    "- Look at variables that don't actually represent numeric values to think about encoding. (fips, regionidcity, regionidzip, regionidcounty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['age'] = 2017 - df.yearbuilt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:,'latitude'] = df.loc[:,'latitude']/1e6\n",
    "df.loc[:,'longitude'] = df.loc[:,'longitude']/1e6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "City ID and County: Is there any cross-over or is city purely a subset of county? \n",
    "\n",
    "Count the number of counties each city is located in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = pd.DataFrame(pd.crosstab(df.regionidcity, df.regionidcounty))\n",
    "s = ct.astype(bool).sum(axis=1)\n",
    "s = s.where(s>1).dropna()\n",
    "pd.crosstab(df[df.regionidcity.isin(list(s.index))].regionidcity, df[df.regionidcity.isin(list(s.index))].regionidcounty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking a look at these, I can see that when there are multiple counties, there is clearly a dominant county and only a handful of properties in the other. I will 'fix' the anomalies to be in what is likely the correct county. I'll test it here, but will need to implement above before we do all the prepping. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.regionidcity.isin([5465.0,12447.0,12520.0]), 'regionidcounty'] = 3101.0\n",
    "df.loc[df.regionidcity.isin([10608.0,15237.0,18874.0,44833.0]), 'regionidcounty'] = 1286.0\n",
    "df.loc[df.regionidcity==41673.0, 'regionidcounty'] = 2061.0\n",
    "df.regionidcounty.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Looking at the counts for each county, it seems reasonable to not split county 2061 geographically much more than that.  \n",
    "- County 1286 can probably be split more effectively: city 16764 and all others \n",
    "- County 3101 can definitedly be split more effectively: city 12447, 5534, 46298, 40227, and all others\n",
    "\n",
    "However, I'm going to wait to do this. I will first run some statistical tests to see if there are cities and zips that have significantly different logerror from the rest of the properties. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.loc[(df['regionidcity']==12447) | (df['regionidcity']==5534) | (df['regionidcity']==40227) | (df['regionidcity']==46298) | (df['regionidcity']==16764), 'cityid'] = df['regionidcity']\n",
    "# df.cityid.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# have to do float first because of an issue with 0.0, then int, then string to ensure no decimals in the string.  \n",
    "# df['cityid'] = df.cityid.astype(float).astype(int).astype(str)\n",
    "df['regionidcity'] = df.regionidcity.astype(float).astype(int).astype(str)\n",
    "df['regionidzip'] = df.regionidzip.astype(float).astype(int).astype(str)\n",
    "df['regionidcounty'] = df.regionidcounty.astype(float).astype(int).astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['loc_id'] = df.regionidcounty + '_' + df.cityid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up remaining columns\n",
    "df_prepped = df.drop(columns=(['id','parcelid','assessmentyear','propertycountylandusecode',\n",
    "                               'propertylandusedesc','transactiondate', 'propertylandusetypeid',\n",
    "                               'finishedsquarefeet12', 'taxvaluedollarcnt', 'fips',\n",
    "                               'yearbuilt', 'rawcensustractandblock', 'censustractandblock', 'roomcnt', \n",
    "                               'calculatedbathnbr','taxamount','calculatedfinishedsquarefeet',\n",
    "                               'landtaxvaluedollarcnt','structuretaxvaluedollarcnt',\n",
    "                               'bedroomcnt','bathroomcnt','fullbathcnt'\n",
    "                              ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prepped.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df_prepped, test_size=.30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Uniform Scaler\n",
    "when we the space between doesn't matter as much as order does, a uniform scaler is a good choice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_uniform(train, test, column_list):\n",
    "    scaler = QuantileTransformer(output_distribution='uniform', random_state=123)\n",
    "    train_scaled = pd.DataFrame(scaler.fit_transform(train[column_list]), \n",
    "                                columns = column_list, \n",
    "                                index = train.index)\n",
    "    train.drop(columns=column_list, inplace=True)\n",
    "    train = train.join(train_scaled)\n",
    "    \n",
    "    test_scaled = pd.DataFrame(scaler.transform(test[column_list]), \n",
    "                                columns = column_list, \n",
    "                                index = test.index)\n",
    "    test.drop(columns=column_list, inplace=True)\n",
    "    test = test.join(test_scaled)\n",
    "    \n",
    "    return train, test, scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create MinMax Scaler\n",
    "When we want to preserve distance but want to be within bounds, a min-max scaler is a good choice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_minmax(train, test, column_list):\n",
    "    scaler = MinMaxScaler(feature_range=(0,1))\n",
    "    train_scaled = pd.DataFrame(scaler.fit_transform(train[column_list]), \n",
    "                                columns = column_list, \n",
    "                                index = train.index)\n",
    "    train.drop(columns=column_list, inplace=True)\n",
    "    train = train.join(train_scaled)\n",
    "    \n",
    "    test_scaled = pd.DataFrame(scaler.transform(test[column_list]), \n",
    "                                columns = column_list, \n",
    "                                index = test.index)\n",
    "    test.drop(columns=column_list, inplace=True)\n",
    "    test = test.join(test_scaled)\n",
    "    \n",
    "    return train, test, scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will scale square feet, dollar per square foot, tax rate, beds & baths using a uniform scaler as that will help minimize the impact extreme outliers will have. \n",
    "\n",
    "For latitude, longitude and age, we want to preserve the distance between. We want 1876 to be futher away from the next oldest house of 1900 than 1900 is from the next oldest house of 1901. For this reason, we will use a min-max scaler. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_list1 = ['lotsizesquarefeet','structure_dollar_per_sqft','land_dollar_per_sqft','living_area_sqft','tax_rate','bedbath_index']\n",
    "train, test, scaler_uniform = scale_uniform(train, test, column_list1)\n",
    "\n",
    "column_list2 = ['latitude','longitude','age']\n",
    "train, test, scaler_minmax = scale_minmax(train, test, column_list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_prepped.describe().T\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster\n",
    "\n",
    "### K-Means\n",
    "\n",
    "#### Elbow Method to determine best 'K'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Let's first cluster by geolocation of latitude and longitude along with dollar per square foot (land and structure) and tax rate.\n",
    "\n",
    "2. Then we will cluster by lot size, living area, beds and baths, and age. \n",
    "\n",
    "##### Clustering 1: location, dollar/sqft, tax rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster1_cols = ['latitude','longitude','land_dollar_per_sqft','structure_dollar_per_sqft']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute and plot the sum squared distances of each sample to closest cluster center at each k-value.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_k(cluster_df, ks):\n",
    "    sse = []\n",
    "    for k in ks:\n",
    "        kmeans = KMeans(n_clusters=k, n_init=1, max_iter=100, random_state=123)\n",
    "        kmeans.fit(cluster_df)\n",
    "\n",
    "        # inertia: Sum of squared distances of samples to their closest cluster center.\n",
    "        sse.append(kmeans.inertia_)\n",
    "\n",
    "    # print(pd.DataFrame(dict(k=ks, sse=sse)))\n",
    "\n",
    "    p = plt.plot(ks, sse, 'bx-')\n",
    "    p = plt.xlabel('k')\n",
    "    p = plt.ylabel('SSE')\n",
    "    p = plt.title('The Elbow Method to find the optimal k')\n",
    "\n",
    "    compare_df = pd.DataFrame(dict(k=ks, sse=sse)).assign(change_in_sse=lambda df: df.sse.diff())\n",
    "    return compare_df, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_k(cluster_df = train[cluster1_cols], ks = range(1,13))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would say 6 or 8 is hwere the bottom of the elbow sits.  \n",
    "Let's compare k=6 vs. k=8. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_clusters(cluster_df, x_column, y_column, z_column, k1, k2):\n",
    "    estimators = [(str(k1)+' Clusters', KMeans(n_clusters=k1, n_init=1, max_iter=100, random_state=123)),\n",
    "                  (str(k2)+' Clusters', KMeans(n_clusters=k2, n_init=1, max_iter=100, random_state=123))]\n",
    "    \n",
    "    fig, axs = plt.subplots(1, 2, figsize=(14, 6), subplot_kw={'projection': '3d'})\n",
    "    \n",
    "    for ax, (title, kmeans) in zip(axs, estimators):\n",
    "        # fit the kmeans object\n",
    "        kmeans.fit(cluster_df)\n",
    "        \n",
    "        labels = kmeans.labels_\n",
    "        \n",
    "        ax.scatter(cluster_df[x_column], \n",
    "                   cluster_df[y_column],\n",
    "                   cluster_df[z_column],\n",
    "                   c=labels.astype(np.float), edgecolor='k')\n",
    "        ax.set(xticklabels=[], yticklabels=[], zticklabels=[])\n",
    "        ax.set(xlabel=x_column, ylabel=y_column, zlabel=z_column)\n",
    "        ax.set(title=title)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_clusters(cluster_df=train[cluster1_cols],\n",
    "                x_column='latitude', y_column='land_dollar_per_sqft', z_column='longitude',\n",
    "                k1=6, k2=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_clusters(cluster_df=train[cluster1_cols],\n",
    "                x_column='latitude', y_column='structure_dollar_per_sqft', z_column='longitude',\n",
    "                k1=6, k2=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_clusters(cluster_df=train[cluster1_cols],\n",
    "                x_column='latitude', y_column='structure_dollar_per_sqft', z_column='land_dollar_per_sqft',\n",
    "                k1=6, k2=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As latitude moves east (left on the chart), we can see the land dollar per sqft and structure dollar per sqft increases, indicated by the slope upward as you move back and to the left. (low latitude, high structure dollar per sqft, high land dollar per sqft. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to go with 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_k_clusters(train, test, cluster_feature_id, cluster_cols, k):\n",
    "    kmeans = KMeans(n_clusters=k, n_init=1, max_iter=100, random_state=123)\n",
    "    kmeans.fit(train[cluster_cols])\n",
    "    cluster_feature = 'cluster'+str(cluster_feature_id)+'_id'\n",
    "    train[cluster_feature] = kmeans.predict(train[cluster_cols])\n",
    "    test[cluster_feature] = kmeans.predict(test[cluster_cols])\n",
    "    return train, test, kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, kmeans1 = create_k_clusters(train, test, cluster_feature_id = 1, cluster_cols = cluster1_cols, k=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.rename(index=str, columns={'cluster1_id': 'cluster_loc'}, inplace=True)\n",
    "test.rename(index=str, columns={'cluster1_id': 'cluster_loc'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['cluster'] = kmeans1.labels_\n",
    "train.cluster = 'cluster_' + (train.cluster + 1).astype('str')\n",
    "for i in range(0,len(cluster1_cols)):\n",
    "    for j in range(0, len(cluster1_cols)):\n",
    "        sns.relplot(data=train, x=cluster1_cols[i], y=cluster1_cols[j], hue='cluster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.cluster_loc.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.cluster_loc.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Clustering 2: size fields and age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster2_cols = ['lotsizesquarefeet', 'living_area_sqft', 'bedbath_index','age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_k(cluster_df = train[cluster2_cols], ks = range(1,13))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare 5 vs. 7 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_clusters(cluster_df=train[cluster2_cols],\n",
    "                x_column='lotsizesquarefeet', y_column='living_area_sqft', z_column='bedbath_index',\n",
    "                k1=5, k2=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_clusters(cluster_df=train[cluster2_cols],\n",
    "                x_column='lotsizesquarefeet', y_column='living_area_sqft', z_column='age',\n",
    "                k1=5, k2=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_clusters(cluster_df=train[cluster2_cols],\n",
    "                x_column='lotsizesquarefeet', y_column='bedbath_index', z_column='age',\n",
    "                k1=5, k2=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_clusters(cluster_df=train[cluster2_cols],\n",
    "                x_column='living_area_sqft', y_column='bedbath_index', z_column='age',\n",
    "                k1=5, k2=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to go with 7 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, kmeans2 = create_k_clusters(train, test, cluster_feature_id = 2, cluster_cols = cluster2_cols, k=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.rename(index=str, columns={'cluster2_id': 'cluster_home'}, inplace=True)\n",
    "test.rename(index=str, columns={'cluster2_id': 'cluster_home'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['cluster'] = kmeans2.labels_\n",
    "train.cluster = 'cluster_' + (train.cluster + 1).astype('str')\n",
    "\n",
    "for i in range(0,len(cluster2_cols)):\n",
    "    for j in range(0, len(cluster2_cols)):\n",
    "        sns.relplot(data=train, x=cluster2_cols[i], y=cluster2_cols[j], hue='cluster')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Which variables and clusters give information about logerror?\n",
    "\n",
    "First, I need to confirm that 'logerror' is normally distributed, to determine if I can run t-tests to test the differences in means across different clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(train.logerror, bins=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test:** Home driven clusters\n",
    "\n",
    "First, let's look at the mean log error by cluster id. We will do this for both the train and test as more of a data quality check...to confirm that our cluster id's are showing similar results in both samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.DataFrame(train.groupby(['cluster_home'])['logerror'].mean().reset_index()))\n",
    "print(pd.DataFrame(test.groupby(['cluster_home'])['logerror'].mean().reset_index()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the different in the mean logerror for each cluster vs all others. When the p-value is < .05 then we keep the cluster id, else we replace the cluster id with -1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "import numpy as np\n",
    "\n",
    "less_significant_clusters = []\n",
    "\n",
    "for i in range(0,max(train.cluster_home)+1):\n",
    "    stat, pval = sp.stats.ttest_ind(\n",
    "        train[train.cluster_home == i].logerror.dropna(),\n",
    "        train[train.cluster_home != i].logerror.dropna())\n",
    "    if pval > .05:\n",
    "        less_significant_clusters = less_significant_clusters + [i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.cluster_home = train.cluster_home.replace(less_significant_clusters, -1)\n",
    "test.cluster_home = test.cluster_home.replace(less_significant_clusters, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.cluster_home.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.cluster_home.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test:** Location driven clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(train.groupby(['cluster_loc'])['logerror'].mean().reset_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "less_significant_clusters = []\n",
    "\n",
    "for i in range(0,max(train.cluster_loc)+1):\n",
    "    stat, pval = sp.stats.ttest_ind(\n",
    "        train[train.cluster_loc == i].logerror.dropna(),\n",
    "        train[train.cluster_loc != i].logerror.dropna())\n",
    "    if pval > .05:\n",
    "        less_significant_clusters = less_significant_clusters + [i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "less_significant_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.cluster_loc = train.cluster_loc.replace(less_significant_clusters, -1)\n",
    "test.cluster_loc = test.cluster_loc.replace(less_significant_clusters, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test:** is_taxdelinquent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(train.groupby(['is_taxdelinquent'])['logerror'].mean().reset_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats, pval = sp.stats.ttest_ind(\n",
    "    train[train.is_taxdelinquent == 0].logerror.dropna(),\n",
    "    train[train.is_taxdelinquent == 1].logerror.dropna())\n",
    "\n",
    "if pval > .05:\n",
    "    train.drop(columns=['is_taxdelinquent'], inplace=True)\n",
    "    test.drop(columns=['is_taxdelinquent'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pval)\n",
    "# if pval < 0.05 then the column should still exist: \n",
    "'is_taxdelinquent' in train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results:** is_taxdelinquent was correctly left as feature. \n",
    "\n",
    "_________________________\n",
    "\n",
    "**Test:** has_pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(train.groupby(['has_pool'])['logerror'].mean().reset_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats, pval = sp.stats.ttest_ind(\n",
    "    train[train.has_pool == 0].logerror.dropna(),\n",
    "    train[train.has_pool == 1].logerror.dropna())\n",
    "\n",
    "if pval > .05:\n",
    "    train.drop(columns=['has_pool'], inplace=True)\n",
    "    test.drop(columns=['has_pool'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pval)\n",
    "# verify column exists if pval < 0.05, and not if greater\n",
    "'has_pool' in train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results:** has_pool was correctly left as feature\n",
    "\n",
    "____________________________ \n",
    "\n",
    "**Test:** has_fireplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(train.groupby(['has_fireplace'])['logerror'].mean().reset_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats, pval = sp.stats.ttest_ind(\n",
    "    train[train.has_fireplace == 0].logerror.dropna(),\n",
    "    train[train.has_fireplace == 1].logerror.dropna())\n",
    "\n",
    "if pval > .05:\n",
    "    train.drop(columns=['has_fireplace'], inplace=True)\n",
    "    test.drop(columns=['has_fireplace'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pval)\n",
    "# verify column exists if pval < 0.05, and not if greater\n",
    "'has_fireplace' in train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results:** has_fireplace was correctly removed\n",
    "_______________________\n",
    "\n",
    "**Test:** has_garage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(train.groupby(['has_garage'])['logerror'].mean().reset_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats, pval = sp.stats.ttest_ind(\n",
    "    train[train.has_garage == 0].logerror.dropna(),\n",
    "    train[train.has_garage == 1].logerror.dropna())\n",
    "\n",
    "if pval > .05:\n",
    "    train.drop(columns=['has_garage'], inplace=True)\n",
    "    test.drop(columns=['has_garage'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pval)\n",
    "\n",
    "# verify column exists if pval < 0.05, and not if greater\n",
    "'has_garage' in train.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results:** has_garage was correctly left as a feature\n",
    "\n",
    "_______________________\n",
    "\n",
    "**Clean up remaining features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_remove = ['tax_rate','regionidcity','regionidzip']\n",
    "non_cluster_features = ['lotsizesquarefeet', 'structure_dollar_per_sqft', 'land_dollar_per_sqft',\n",
    "                        'living_area_sqft', 'bedbath_index', 'latitude', 'longitude', 'age']\n",
    "train_no_clusters = train[non_cluster_features+['regionidcounty','logerror']]\n",
    "test_no_clusters = test[non_cluster_features+['regionidcounty','logerror']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_remove = cols_to_remove + non_cluster_features\n",
    "train_clusters = train.drop(columns=cols_to_remove)\n",
    "test_clusters = test.drop(columns=cols_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_no_clusters.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_clusters.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode\n",
    "\n",
    "Which columns are of numeric format but represent classes or categories?\n",
    "fips\n",
    "rawcensustractandblock\n",
    "regionidcity\n",
    "regionidcounty\n",
    "regionidzip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(train, test, col_name):\n",
    "  \n",
    "    encoded_values = sorted(list(train[col_name].unique()))\n",
    "    columns = [col_name + '_' + str(val) for val in encoded_values]\n",
    "\n",
    "    # Integer Encoding\n",
    "    int_encoder = LabelEncoder()\n",
    "    train.encoded = int_encoder.fit_transform(train[col_name])\n",
    "    test.encoded = int_encoder.transform(test[col_name])\n",
    "\n",
    "    # create 2D np arrays of the encoded variable (in train and test)\n",
    "    train_array = np.array(train.encoded).reshape(len(train.encoded),1)\n",
    "    test_array = np.array(test.encoded).reshape(len(test.encoded),1)\n",
    "    \n",
    "    # One Hot Encoding\n",
    "    ohe = OneHotEncoder(sparse=False, categories='auto')\n",
    "    train_ohe = ohe.fit_transform(train_array)\n",
    "    test_ohe = ohe.transform(test_array)\n",
    "\n",
    "    # Turn the array of new values into a data frame with columns names being the values\n",
    "    # and index matching that of train/test\n",
    "    # then merge the new dataframe with the existing train/test dataframe\n",
    "    train_encoded = pd.DataFrame(data=train_ohe,\n",
    "                            columns=columns, index=train.index)\n",
    "    train = train.join(train_encoded)\n",
    "\n",
    "    test_encoded = pd.DataFrame(data=test_ohe,\n",
    "                               columns=columns, index=test.index)\n",
    "    test = test.join(test_encoded)\n",
    "\n",
    "    return train, test, int_encoder, ohe\n",
    "\n",
    "def one_hot_encode(train, test, col_name):\n",
    "  \n",
    "    encoded_values = sorted(list(train[col_name].unique()))\n",
    "    columns = [col_name + '_' + str(val) for val in encoded_values]\n",
    "\n",
    "    # create 2D np arrays of the encoded variable (in train and test)\n",
    "    train_array = np.array(train[col_name]).reshape(len(train[col_name]),1)\n",
    "    test_array = np.array(test[col_name]).reshape(len(test[col_name]),1)\n",
    "    \n",
    "    # One Hot Encoding\n",
    "    ohe = OneHotEncoder(sparse=False, categories='auto')\n",
    "    train_ohe = ohe.fit_transform(train_array)\n",
    "    test_ohe = ohe.transform(test_array)\n",
    "\n",
    "    # Turn the array of new values into a data frame with columns names being the values\n",
    "    # and index matching that of train/test\n",
    "    # then merge the new dataframe with the existing train/test dataframe\n",
    "    train_encoded = pd.DataFrame(data=train_ohe,\n",
    "                            columns=columns, index=train.index)\n",
    "    train = train.join(train_encoded)\n",
    "\n",
    "    test_encoded = pd.DataFrame(data=test_ohe,\n",
    "                               columns=columns, index=test.index)\n",
    "    test = test.join(test_encoded)\n",
    "\n",
    "    return train, test, ohe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_clusters, test_clusters, ohe_loc = one_hot_encode(train_clusters, test_clusters, 'cluster_loc')\n",
    "train_clusters, test_clusters, ohe_home = one_hot_encode(train_clusters, test_clusters, 'cluster_home')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build 3 models, 1 for each county\n",
    "Try with clusters and then try with original features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_clusters.regionidcounty.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate the clusters dataframes by county"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_3101_c = train_clusters[train_clusters.regionidcounty=='3101']\n",
    "test_3101_c = test_clusters[test_clusters.regionidcounty=='3101']\n",
    "\n",
    "train_1286_c = train_clusters[train_clusters.regionidcounty=='1286']\n",
    "test_1286_c = test_clusters[test_clusters.regionidcounty=='1286']\n",
    "\n",
    "train_2061_c = train_clusters[train_clusters.regionidcounty=='2061']\n",
    "test_2061_c = test_clusters[test_clusters.regionidcounty=='2061']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate the non-clusters dataframes by county"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_3101_nc = train_no_clusters[train_no_clusters.regionidcounty=='3101']\n",
    "test_3101_nc = test_no_clusters[test_no_clusters.regionidcounty=='3101']\n",
    "\n",
    "train_1286_nc = train_no_clusters[train_no_clusters.regionidcounty=='1286']\n",
    "test_1286_nc = test_no_clusters[test_no_clusters.regionidcounty=='1286']\n",
    "\n",
    "train_2061_nc = train_no_clusters[train_no_clusters.regionidcounty=='2061']\n",
    "test_2061_nc = test_no_clusters[test_no_clusters.regionidcounty=='2061']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have encoded and split by county, we can remove some other columns to have our final X with features.\n",
    "We only need a y_train and y_test for each county, not separated by the features, obviously. \n",
    "\n",
    "X dataframes for the cluster features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ['cluster_loc', 'cluster_home', 'cluster_loc_-1', 'cluster_home_-1', 'logerror', 'regionidcounty']\n",
    "\n",
    "X_train_3101_c = train_3101_c.drop(columns=cols_to_drop)\n",
    "X_test_3101_c = test_3101_c.drop(columns=cols_to_drop)\n",
    "\n",
    "X_train_1286_c = train_1286_c.drop(columns=cols_to_drop)\n",
    "X_test_1286_c = test_1286_c.drop(columns=cols_to_drop)\n",
    "\n",
    "X_train_2061_c = train_2061_c.drop(columns=cols_to_drop)\n",
    "X_test_2061_c = test_2061_c.drop(columns=cols_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X dataframes for the non-cluster features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ['logerror', 'regionidcounty']\n",
    "\n",
    "X_train_3101_nc = train_3101_nc.drop(columns=cols_to_drop)\n",
    "X_test_3101_nc = test_3101_nc.drop(columns=cols_to_drop)\n",
    "\n",
    "X_train_1286_nc = train_1286_nc.drop(columns=cols_to_drop)\n",
    "X_test_1286_nc = test_1286_nc.drop(columns=cols_to_drop)\n",
    "\n",
    "X_train_2061_nc = train_2061_nc.drop(columns=cols_to_drop)\n",
    "X_test_2061_nc = test_2061_nc.drop(columns=cols_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_3101 = train_3101_c[['logerror']]\n",
    "y_test_3101 = test_3101_c[['logerror']]\n",
    "\n",
    "y_train_1286 = train_1286_c[['logerror']]\n",
    "y_test_1286 = test_1286_c[['logerror']]\n",
    "\n",
    "y_train_2061 = train_2061_c[['logerror']]\n",
    "y_test_2061 = test_2061_c[['logerror']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import SGDRegressor, LassoCV\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### county: 3101\n",
    "\n",
    "#### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "se = y_train_3101.logerror * y_train_3101.logerror\n",
    "mse = se.mean()\n",
    "rmse = mse**1/2\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering Features\n",
    "\n",
    "##### Linear Support Vector Regressor from sklearn.svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = LinearSVR(random_state=123, tol=1e-5, loss='squared_epsilon_insensitive', fit_intercept=False, dual=False)\n",
    "regr.fit(X_train_3101_c, y_train_3101)\n",
    "y_pred_3101 = regr.predict(X_train_3101_c)\n",
    "print(mean_squared_error(y_train_3101, y_pred_3101)**1/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Support Gradient Descent Regressor from sklearn.linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = SGDRegressor(fit_intercept=False, max_iter=1000, random_state=123)\n",
    "sgd.fit(X_train_3101_c, y_train_3101)\n",
    "y_pred_3101 = sgd.predict(X_train_3101_c)\n",
    "mean_squared_error(y_train_3101, y_pred_3101)**1/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lasso with Cross Validation from sklearn.linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = LassoCV(fit_intercept=False)\n",
    "lasso.fit(X_train_3101_c, y_train_3101)\n",
    "y_pred_3101 = lasso.predict(X_train_3101_c)\n",
    "mean_squared_error(y_train_3101, y_pred_3101)**1/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Decision Tree Regressor from sklearn.tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeRegressor(random_state=123)\n",
    "dt.fit(X_train_3101_c, y_train_3101)\n",
    "y_pred_3101 = dt.predict(X_train_3101_c)\n",
    "mean_squared_error(y_train_3101, y_pred_3101)**1/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### non-clustering features\n",
    "\n",
    "##### Linear Support Vector Regressor from sklearn.svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = LinearSVR(random_state=123, tol=1e-5, loss='squared_epsilon_insensitive', fit_intercept=False, dual=False)\n",
    "regr.fit(X_train_3101_nc, y_train_3101)\n",
    "y_pred_3101 = regr.predict(X_train_3101_nc)\n",
    "print(mean_squared_error(y_train_3101, y_pred_3101)**1/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Support Gradient Descent Regressor from sklearn.linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = SGDRegressor(fit_intercept=False, max_iter=1000, random_state=123)\n",
    "sgd.fit(X_train_3101_nc, y_train_3101)\n",
    "y_pred_3101 = sgd.predict(X_train_3101_nc)\n",
    "mean_squared_error(y_train_3101, y_pred_3101)**1/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lasso with Cross Validation from sklearn.linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = LassoCV(fit_intercept=False)\n",
    "lasso.fit(X_train_3101_nc, y_train_3101)\n",
    "y_pred_3101 = lasso.predict(X_train_3101_nc)\n",
    "mean_squared_error(y_train_3101, y_pred_3101)**1/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Decision Tree Regressor from sklearn.tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeRegressor(random_state=123)\n",
    "dt.fit(X_train_3101_nc, y_train_3101)\n",
    "y_pred_3101 = dt.predict(X_train_3101_nc)\n",
    "mean_squared_error(y_train_3101, y_pred_3101)**1/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WOW!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### county: 3101\n",
    "\n",
    "#### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "se = y_train_3101.logerror * y_train_3101.logerror\n",
    "mse = se.mean()\n",
    "rmse = mse**1/2\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering Features\n",
    "\n",
    "##### Linear Support Vector Regressor from sklearn.svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = LinearSVR(random_state=123, tol=1e-5, loss='squared_epsilon_insensitive', fit_intercept=False, dual=False)\n",
    "regr.fit(X_train_3101_c, y_train_3101)\n",
    "y_pred_3101 = regr.predict(X_train_3101_c)\n",
    "print(mean_squared_error(y_train_3101, y_pred_3101)**1/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Support Gradient Descent Regressor from sklearn.linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = SGDRegressor(fit_intercept=False, max_iter=1000, random_state=123)\n",
    "sgd.fit(X_train_3101_c, y_train_3101)\n",
    "y_pred_3101 = sgd.predict(X_train_3101_c)\n",
    "mean_squared_error(y_train_3101, y_pred_3101)**1/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lasso with Cross Validation from sklearn.linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = LassoCV(fit_intercept=False)\n",
    "lasso.fit(X_train_3101_c, y_train_3101)\n",
    "y_pred_3101 = lasso.predict(X_train_3101_c)\n",
    "mean_squared_error(y_train_3101, y_pred_3101)**1/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Decision Tree Regressor from sklearn.tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeRegressor(random_state=123)\n",
    "dt.fit(X_train_3101_c, y_train_3101)\n",
    "y_pred_3101 = dt.predict(X_train_3101_c)\n",
    "mean_squared_error(y_train_3101, y_pred_3101)**1/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### non-clustering features\n",
    "\n",
    "##### Linear Support Vector Regressor from sklearn.svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = LinearSVR(random_state=123, tol=1e-5, loss='squared_epsilon_insensitive', fit_intercept=False, dual=False)\n",
    "regr.fit(X_train_3101_nc, y_train_3101)\n",
    "y_pred_3101 = regr.predict(X_train_3101_nc)\n",
    "print(mean_squared_error(y_train_3101, y_pred_3101)**1/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Support Gradient Descent Regressor from sklearn.linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = SGDRegressor(fit_intercept=False, max_iter=1000, random_state=123)\n",
    "sgd.fit(X_train_3101_nc, y_train_3101)\n",
    "y_pred_3101 = sgd.predict(X_train_3101_nc)\n",
    "mean_squared_error(y_train_3101, y_pred_3101)**1/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lasso with Cross Validation from sklearn.linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = LassoCV(fit_intercept=False)\n",
    "lasso.fit(X_train_3101_nc, y_train_3101)\n",
    "y_pred_3101 = lasso.predict(X_train_3101_nc)\n",
    "mean_squared_error(y_train_3101, y_pred_3101)**1/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Decision Tree Regressor from sklearn.tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeRegressor(random_state=123)\n",
    "dt.fit(X_train_3101_nc, y_train_3101)\n",
    "y_pred_3101 = dt.predict(X_train_3101_nc)\n",
    "mean_squared_error(y_train_3101, y_pred_3101)**1/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WOW!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### county: 1286\n",
    "\n",
    "#### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "se = y_train_1286.logerror * y_train_1286.logerror\n",
    "mse = se.mean()\n",
    "rmse = mse**1/2\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering Features\n",
    "\n",
    "##### Linear Support Vector Regressor from sklearn.svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = LinearSVR(random_state=123, tol=1e-5, loss='squared_epsilon_insensitive', fit_intercept=False, dual=False)\n",
    "regr.fit(X_train_1286_c, y_train_1286)\n",
    "y_pred_1286 = regr.predict(X_train_1286_c)\n",
    "print(mean_squared_error(y_train_1286, y_pred_1286)**1/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Support Gradient Descent Regressor from sklearn.linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = SGDRegressor(fit_intercept=False, max_iter=1000, random_state=123)\n",
    "sgd.fit(X_train_1286_c, y_train_1286)\n",
    "y_pred_1286 = sgd.predict(X_train_1286_c)\n",
    "mean_squared_error(y_train_1286, y_pred_1286)**1/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lasso with Cross Validation from sklearn.linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = LassoCV(fit_intercept=False)\n",
    "lasso.fit(X_train_1286_c, y_train_1286)\n",
    "y_pred_1286 = lasso.predict(X_train_1286_c)\n",
    "mean_squared_error(y_train_1286, y_pred_1286)**1/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Decision Tree Regressor from sklearn.tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeRegressor(random_state=123)\n",
    "dt.fit(X_train_1286_c, y_train_1286)\n",
    "y_pred_1286 = dt.predict(X_train_1286_c)\n",
    "mean_squared_error(y_train_1286, y_pred_1286)**1/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### non-clustering features\n",
    "\n",
    "##### Linear Support Vector Regressor from sklearn.svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = LinearSVR(random_state=123, tol=1e-5, loss='squared_epsilon_insensitive', fit_intercept=False, dual=False)\n",
    "regr.fit(X_train_1286_nc, y_train_1286)\n",
    "y_pred_1286 = regr.predict(X_train_1286_nc)\n",
    "print(mean_squared_error(y_train_1286, y_pred_1286)**1/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Support Gradient Descent Regressor from sklearn.linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = SGDRegressor(fit_intercept=False, max_iter=1000, random_state=123)\n",
    "sgd.fit(X_train_1286_nc, y_train_1286)\n",
    "y_pred_3101 = sgd.predict(X_train_1286_nc)\n",
    "mean_squared_error(y_train_1286, y_pred_1286)**1/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lasso with Cross Validation from sklearn.linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = LassoCV(fit_intercept=False)\n",
    "lasso.fit(X_train_1286_nc, y_train_1286)\n",
    "y_pred_1286 = lasso.predict(X_train_1286_nc)\n",
    "mean_squared_error(y_train_1286, y_pred_1286)**1/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Decision Tree Regressor from sklearn.tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeRegressor(random_state=123)\n",
    "dt.fit(X_train_1286_nc, y_train_1286)\n",
    "y_pred_1286 = dt.predict(X_train_1286_nc)\n",
    "mean_squared_error(y_train_1286, y_pred_1286)**1/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WOW again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### county: 2061\n",
    "\n",
    "#### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "se = y_train_2061.logerror * y_train_2061.logerror\n",
    "mse = se.mean()\n",
    "rmse = mse**1/2\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering Features\n",
    "\n",
    "##### Linear Support Vector Regressor from sklearn.svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = LinearSVR(random_state=123, tol=1e-5, loss='squared_epsilon_insensitive', fit_intercept=False, dual=False)\n",
    "regr.fit(X_train_2061_c, y_train_2061)\n",
    "y_pred_2061 = regr.predict(X_train_2061_c)\n",
    "print(mean_squared_error(y_train_2061, y_pred_2061)**1/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Support Gradient Descent Regressor from sklearn.linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = SGDRegressor(fit_intercept=False, max_iter=1000, random_state=123)\n",
    "sgd.fit(X_train_2061_c, y_train_2061)\n",
    "y_pred_2061 = sgd.predict(X_train_2061_c)\n",
    "mean_squared_error(y_train_2061, y_pred_2061)**1/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lasso with Cross Validation from sklearn.linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = LassoCV(fit_intercept=False)\n",
    "lasso.fit(X_train_2061_c, y_train_2061)\n",
    "y_pred_2061 = lasso.predict(X_train_2061_c)\n",
    "mean_squared_error(y_train_2061, y_pred_2061)**1/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Decision Tree Regressor from sklearn.tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeRegressor(random_state=123)\n",
    "dt.fit(X_train_2061_c, y_train_2061)\n",
    "y_pred_2061 = dt.predict(X_train_2061_c)\n",
    "mean_squared_error(y_train_2061, y_pred_2061)**1/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### non-clustering features\n",
    "\n",
    "##### Linear Support Vector Regressor from sklearn.svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = LinearSVR(random_state=123, tol=1e-5, loss='squared_epsilon_insensitive', fit_intercept=False, dual=False)\n",
    "regr.fit(X_train_2061_nc, y_train_2061)\n",
    "y_pred_2061 = regr.predict(X_train_2061_nc)\n",
    "print(mean_squared_error(y_train_2061, y_pred_2061)**1/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Support Gradient Descent Regressor from sklearn.linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = SGDRegressor(fit_intercept=False, max_iter=1000, random_state=123)\n",
    "sgd.fit(X_train_2061_nc, y_train_2061)\n",
    "y_pred_2061 = sgd.predict(X_train_2061_nc)\n",
    "mean_squared_error(y_train_2061, y_pred_2061)**1/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lasso with Cross Validation from sklearn.linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = LassoCV(fit_intercept=False)\n",
    "lasso.fit(X_train_2061_nc, y_train_2061)\n",
    "y_pred_2061 = lasso.predict(X_train_2061_nc)\n",
    "mean_squared_error(y_train_2061, y_pred_2061)**1/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Decision Tree Regressor from sklearn.tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeRegressor(random_state=123)\n",
    "dt.fit(X_train_2061_nc, y_train_2061)\n",
    "y_pred_2061 = dt.predict(X_train_2061_nc)\n",
    "mean_squared_error(y_train_2061, y_pred_2061)**1/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
